{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a19dc1a",
   "metadata": {
    "tags": [
     "tools/dask-cuda",
     "aws/ec2",
     "aws/sagemaker",
     "azure/azure-vm",
     "azure/ml",
     "gcp/compute-engine",
     "gcp/vertex-ai",
     "data-format/csv",
     "library/cudf",
     "library/cupy",
     "library/dask",
     "library/pandas"
    ]
   },
   "source": [
    "# One Billion Row Challenge out of core on multiple GPUs\n",
    "\n",
    "The [One Billion Row Challenge](https://www.morling.dev/blog/one-billion-row-challenge/) is a programming competition aimed at Java developers to write the most efficient code to process a one billion line text file and calculate some metrics. The challenge has inspired solutions in many languages beyond Java including [Python](https://github.com/gunnarmorling/1brc/discussions/62).\n",
    "\n",
    "In this notebook we will explore how we can use RAPIDS to build an efficient solution to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e9a93",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "The input data of the challenge is a ~13GB text file containing one billion lines of temperature measurements. The file is structured with one measurement per file with the name of the weather station and the measurement separated by a semicolon.\n",
    "\n",
    "```text\n",
    "Hamburg;12.0\n",
    "Bulawayo;8.9\n",
    "Palembang;38.8\n",
    "St. John's;15.2\n",
    "Cracow;12.6\n",
    "...\n",
    "```\n",
    "\n",
    "Our goal is to calculate the min, mean, and max temperature per weather station sorted alphabetically by station name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2037a13",
   "metadata": {},
   "source": [
    "## A PyData solution\n",
    "\n",
    "A solution written with popular PyData tools would likely be something along the lines of the following Pandas code (assuming you have enough RAM).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"measurements.txt\",\n",
    "    sep=\";\",\n",
    "    header=None,\n",
    "    names=[\"station\", \"measure\"],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "df = df.groupby(\"station\").agg([\"min\", \"max\", \"mean\"])\n",
    "df.columns = df.columns.droplevel()\n",
    "df = df.sort_values(\"station\")\n",
    "```\n",
    "\n",
    "Here we use `pandas.read_csv()` to open the text file and specify the `;` separator and also set some column names. We also set the engine to `pyarrow` to give us some extra performance out of the box.\n",
    "\n",
    "Then we group the measurements by their station name and calculate the min, max and mean. Finally we sort the grouped dataframe by the station name.\n",
    "\n",
    "Running this on a workstation with a 12-core CPU completes the task in around **4 minutes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc7676",
   "metadata": {},
   "source": [
    "## GPU solution with RAPIDS\n",
    "\n",
    "We can certainly use RAPIDS to speed that up, but if you directly convert the above example from Pandas to cuDF you will run into some [limitations it has with string columns](https://github.com/rapidsai/cudf/issues/13733). Also depending on your GPU you may run into memory limits as cuDF will read the whole dataset into memory.\n",
    "\n",
    "Therefore to solve this with RAPIDS we also need to use Dask to partition the dataset and stream it through GPU memory, then cuDF can process each partition in a performant way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750593c9",
   "metadata": {},
   "source": [
    "### Deploying Dask\n",
    "\n",
    "To run this solution we will need a single machine with one or more GPUs. There are many ways you can get this:\n",
    "\n",
    "- Have a laptop, desktop or workstation with GPUs.\n",
    "- Run VM on the cloud using [AWS EC2](/cloud/aws/ec2), [Google Compute Engine](/cloud/gcp/compute-engine/), [Azure VMs](/cloud/azure/azure-vm/), etc.\n",
    "- Use a managed notebook service like [SageMaker](/cloud/aws/sagemaker/), [Vertex AI](/cloud/gcp/vertex-ai/), [Azure ML](/cloud/azure/azureml/) or [Databricks](/platforms/databricks/).\n",
    "- Run a container in a [Kubernetes cluster with GPUs](/platforms/kubernetes/).\n",
    "\n",
    "Once you have a GPU machine you will need to [install RAPIDS](/local/). You can do this with [pip](https://docs.rapids.ai/install#pip), [conda](https://docs.rapids.ai/install#conda) or [docker](https://docs.rapids.ai/install#docker).\n",
    "\n",
    "Then once you are in your RAPIDS Python environment you can use [dask-cuda](/tools/dask-cuda/) to start a GPU Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e48b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "client = Client(LocalCUDACluster())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc25a17",
   "metadata": {},
   "source": [
    "Creating a `LocalCUDACluster()` inspects the machine and starts one Dask worker for each detected GPU. We then pass that to a Dask client which means that all following code in the notebook will leverage the GPU workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfe48c-47f8-4407-821f-ba1efdcfd9da",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "Before we get started with our problem we need to generate the input data. The 1BRC repo has a [Java implementation](https://github.com/gunnarmorling/1brc/blob/main/src/main/java/dev/morling/onebrc/CreateMeasurements.java) which takes around 15 minutes to generate the file. However we can do this on the GPU using cuDF and CuPy much faster, and we don't need Dask for this because we can generate the data in separate chunks and append them to the output file.\n",
    "\n",
    "Based on the `lookup.csv` table of stations and their mean temperatures we want to generate our file containing `n` rows of random temperatures.\n",
    "\n",
    "To generate each row we choose a random station from the lookup table, then generate a random temperature measurement from a normal distribution around the mean temp. We assume the standard deviation is `10.0` for all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68dcc86b-ddc1-4283-a129-7f33870d4c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import cudf\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98d86b4a-62ed-45e8-9202-8ee4b45f2aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_chunk(filename, chunksize, std, lookup_df):\n",
    "    \"\"\"Generate some sample data based on the lookup table.\"\"\"\n",
    "    df = cudf.DataFrame(\n",
    "        {\n",
    "            # Choose a random station from the lookup table for each row in our output\n",
    "            \"station\": cp.random.randint(0, len(lookup_df) - 1, int(chunksize)),\n",
    "            # Generate a normal distibution around zero for each row in our output\n",
    "            # Because the std is the same for every station we can adjust the mean for each row afterwards\n",
    "            \"measure\": cp.random.normal(0, std, int(chunksize)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Offset each measurement by the station's mean value\n",
    "    df.measure += df.station.map(lookup_df.mean_temp)\n",
    "    # Round the temprature to one decimal place\n",
    "    df.measure = df.measure.round(decimals=1)\n",
    "    # Convert the station index to the station name\n",
    "    df.station = df.station.map(lookup_df.station)\n",
    "\n",
    "    # Append this chunk to the output file\n",
    "    with open(filename, \"a\") as fh:\n",
    "        df.to_csv(fh, sep=\";\", chunksize=10_000_000, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1d27-f890-448a-8841-3cb4547a0a9b",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71484827-cf06-42b8-b635-c95edf0c3380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 1_000_000_000  # Number of rows of data to generate\n",
    "\n",
    "lookup_df = cudf.read_csv(\n",
    "    \"lookup.csv\"\n",
    ")  # Load our lookup table of stations and their mean temperatures\n",
    "std = 10.0  # We assume temperatures are normally distributed with a standard deviation of 10\n",
    "chunksize = 2e8  # Set the number of rows to generate in one go (reduce this if you run into GPU RAM limits)\n",
    "filename = Path(f\"measurements.txt\")  # Choose where to write to\n",
    "filename.unlink() if filename.exists() else None  # Delete the file if it exists already"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4032b-027c-4a9b-a3b0-b8a345b646d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run the data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0285ee3d-9b1c-4dfd-a738-691483fe18a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1 billion rows to data_1b.txt: 100% in 24s (0s remaining)\n",
      "CPU times: user 8.89 s, sys: 16.7 s, total: 25.6 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop over chunks and generate data\n",
    "start = time.time()\n",
    "for i in range(int(n / chunksize)):\n",
    "    # Generate a chunk\n",
    "    generate_chunk(filename, chunksize, std, lookup_df)\n",
    "\n",
    "    # Update the progress bar\n",
    "    percent_complete = int(((i + 1) * chunksize) / n * 100)\n",
    "    time_taken = int(time.time() - start)\n",
    "    time_remaining = int((time_taken / percent_complete) * 100) - time_taken\n",
    "    print(\n",
    "        f\"Writing {int(n / 1e9)} billion rows to {filename}: {percent_complete}% in {time_taken}s ({time_remaining}s remaining)\",\n",
    "        end=\"\\r\",\n",
    "    )\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82f9b0-61f4-4e8c-8312-d97638bd25cf",
   "metadata": {},
   "source": [
    "#### Check the files\n",
    "\n",
    "Now we can verify our dataset is the size we expected and contains rows that follow the format needed by the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7e92fc1-696c-4593-a1df-9490c15d44ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 rapids conda 13G Jan 19 09:23 data_1b.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a4a3e4c-5670-4c6e-91ef-863288aecdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muscat;32.1\n",
      "Kunming;10.8\n",
      "Ho Chi Minh City;36.0\n",
      "Belgrade;19.5\n",
      "Nicosia;18.0\n",
      "Lhasa;-4.0\n",
      "La Paz;5.2\n",
      "Mek'ele;18.2\n",
      "Kuopio;8.3\n",
      "Zagreb;16.5\n"
     ]
    }
   ],
   "source": [
    "!head {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f02767",
   "metadata": {},
   "source": [
    "### Dask + cuDF Solution\n",
    "\n",
    "Now that we have our input data we can write some Dask code that leverages cuDF under the hood to perform the compute operations.\n",
    "\n",
    "First we need to import `dask.dataframe` and tell it to use the `cudf` backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc71c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a792b2",
   "metadata": {},
   "source": [
    "Now we can run our Dask code, which is almost identical to the Pandas code we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff3cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46 s ± 37.3 ms per loop (mean ± std. dev. of 4 runs, 3 loops each)"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 4\n",
    "df = dd.read_csv(\"measurements.txt\", sep=\";\", header=None, names=[\"station\", \"measure\"])\n",
    "df = df.groupby(\"station\").agg([\"min\", \"max\", \"mean\"])\n",
    "df.columns = df.columns.droplevel()\n",
    "df = (\n",
    "    df.compute().to_pandas()\n",
    ")  # We need to switch back to Pandas for the final sort at the time of writing due to rapidsai/cudf#14794\n",
    "df = df.sort_values(\"station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b0979",
   "metadata": {},
   "source": [
    "Running this notebook on a desktop workstation with two NVIDIA RTX 8000 GPUs completes the challenge in around **4 seconds**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
