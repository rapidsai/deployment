{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a19dc1a",
   "metadata": {
    "tags": [
     "tools/dask-cuda",
     "aws/ec2",
     "aws/sagemaker",
     "azure/azure-vm",
     "azure/ml",
     "gcp/compute-engine",
     "gcp/vertex-ai",
     "data-format/csv",
     "library/cudf",
     "library/cupy",
     "library/dask",
     "library/pandas"
    ]
   },
   "source": [
    "# Measuring performance with the One Billion Row Challenge\n",
    "\n",
    "The [One Billion Row Challenge](https://www.morling.dev/blog/one-billion-row-challenge/) is a programming competition aimed at Java developers to write the most efficient code to process a one billion line text file and calculate some metrics. The challenge has inspired solutions in many languages beyond Java including [Python](https://github.com/gunnarmorling/1brc/discussions/62).\n",
    "\n",
    "In this notebook we will explore how we can use RAPIDS to build an efficient solution in Python and how we can use dashboards to understand how performant our code is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e9a93",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "The input data of the challenge is a ~13GB text file containing one billion lines of temperature measurements. The file is structured with one measurement per file with the name of the weather station and the measurement separated by a semicolon.\n",
    "\n",
    "```text\n",
    "Hamburg;12.0\n",
    "Bulawayo;8.9\n",
    "Palembang;38.8\n",
    "St. John's;15.2\n",
    "Cracow;12.6\n",
    "...\n",
    "```\n",
    "\n",
    "Our goal is to calculate the min, mean, and max temperature per weather station sorted alphabetically by station name as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2037a13",
   "metadata": {},
   "source": [
    "## Reference implementation\n",
    "\n",
    "A reference implementation written with popular PyData tools would likely be something along the lines of the following Pandas code (assuming you have enough RAM to fit the data into memory).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"measurements.txt\",\n",
    "    sep=\";\",\n",
    "    header=None,\n",
    "    names=[\"station\", \"measure\"],\n",
    "    engine='pyarrow'\n",
    ")\n",
    "df = df.groupby(\"station\").agg([\"min\", \"max\", \"mean\"])\n",
    "df.columns = df.columns.droplevel()\n",
    "df = df.sort_values(\"station\")\n",
    "```\n",
    "\n",
    "Here we use `pandas.read_csv()` to open the text file and specify the `;` separator and also set some column names. We also set the engine to `pyarrow` to give us some extra performance out of the box.\n",
    "\n",
    "Then we group the measurements by their station name and calculate the min, max and mean. Finally we sort the grouped dataframe by the station name.\n",
    "\n",
    "Running this on a workstation with a 12-core CPU completes the task in around **4 minutes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343c21a-c7bc-491f-a914-4bde648de0d2",
   "metadata": {},
   "source": [
    "## Deploying RAPIDS\n",
    "\n",
    "To run this notebook we will need a machine with one or more GPUs. There are many ways you can get this:\n",
    "\n",
    "- Have a laptop, desktop or workstation with GPUs.\n",
    "- Run a VM on the cloud using [AWS EC2](/cloud/aws/ec2), [Google Compute Engine](/cloud/gcp/compute-engine/), [Azure VMs](/cloud/azure/azure-vm/), etc.\n",
    "- Use a managed notebook service like [SageMaker](/cloud/aws/sagemaker/), [Vertex AI](/cloud/gcp/vertex-ai/), [Azure ML](/cloud/azure/azureml/) or [Databricks](/platforms/databricks/).\n",
    "- Run a container in a [Kubernetes cluster with GPUs](/platforms/kubernetes/).\n",
    "\n",
    "Once you have a GPU machine you will need to [install RAPIDS](/local/). You can do this with [pip](https://docs.rapids.ai/install#pip), [conda](https://docs.rapids.ai/install#conda) or [docker](https://docs.rapids.ai/install#docker).\n",
    "\n",
    "We are also going to use Jupyter Lab with the RAPIDS [nvdashboard extension](https://github.com/rapidsai/jupyterlab-nvdashboard) and the [Dask Lab Extension](https://github.com/dask/dask-labextension) so that we can understand what our machine is doing. If you are using the Docker container these will already be installed for you, otherwise you will need to install them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad91ec7-ed71-4d9d-b7cf-22c5f1185990",
   "metadata": {},
   "source": [
    "### Dashboards\n",
    "\n",
    "Once you have Jupyter up and running with the extensions installed and this notebook downloaded you can open some performance dashboards so we can monitor our hardware as our code runs.\n",
    "\n",
    "Let's start with nvdashboard which has the GPU icon in the left toolbar.\n",
    "\n",
    "![](./nvdashboard-sidebar.png)\n",
    "\n",
    "Start by opening the \"Machine Resources\" table, \"GPU Utilization\" graph and \"GPU Memory\" graph and moving them over to the right hand side.\n",
    "\n",
    "![](./nvdashboard-resources.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfe48c-47f8-4407-821f-ba1efdcfd9da",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Before we get started with our problem we need to generate the input data. The 1BRC repo has a [Java implementation](https://github.com/gunnarmorling/1brc/blob/main/src/main/java/dev/morling/onebrc/CreateMeasurements.java) which takes around 15 minutes to generate the file. \n",
    "\n",
    "If you were to run the Java implementation you would see the CPU get busy but disk bandwidth remain low, suggesting this is a compute bound problem. We can accelerate this on the GPU using cuDF and CuPy.\n",
    "\n",
    "Download the [`lookup.csv`](./lookup.csv) table of stations and their mean temperatures as we will use this to generate our data file containing `n` rows of random temperatures.\n",
    "\n",
    "To generate each row we choose a random station from the lookup table, then generate a random temperature measurement from a normal distribution around the mean temp. We assume the standard deviation is `10.0` for all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dcc86b-ddc1-4283-a129-7f33870d4c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import cudf\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d86b4a-62ed-45e8-9202-8ee4b45f2aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_chunk(filename, chunksize, std, lookup_df):\n",
    "    \"\"\"Generate some sample data based on the lookup table.\"\"\"\n",
    "    df = cudf.DataFrame(\n",
    "        {\n",
    "            # Choose a random station from the lookup table for each row in our output\n",
    "            \"station\": cp.random.randint(0, len(lookup_df) - 1, int(chunksize)),\n",
    "            # Generate a normal distibution around zero for each row in our output\n",
    "            # Because the std is the same for every station we can adjust the mean for each row afterwards\n",
    "            \"measure\": cp.random.normal(0, std, int(chunksize)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Offset each measurement by the station's mean value\n",
    "    df.measure += df.station.map(lookup_df.mean_temp)\n",
    "    # Round the temprature to one decimal place\n",
    "    df.measure = df.measure.round(decimals=1)\n",
    "    # Convert the station index to the station name\n",
    "    df.station = df.station.map(lookup_df.station)\n",
    "\n",
    "    # Append this chunk to the output file\n",
    "    with open(filename, \"a\") as fh:\n",
    "        df.to_csv(fh, sep=\";\", chunksize=10_000_000, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1d27-f890-448a-8841-3cb4547a0a9b",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71484827-cf06-42b8-b635-c95edf0c3380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 1_000_000_000  # Number of rows of data to generate\n",
    "\n",
    "lookup_df = cudf.read_csv(\n",
    "    \"lookup.csv\"\n",
    ")  # Load our lookup table of stations and their mean temperatures\n",
    "std = 10.0  # We assume temperatures are normally distributed with a standard deviation of 10\n",
    "chunksize = 2e8  # Set the number of rows to generate in one go (reduce this if you run into GPU RAM limits)\n",
    "filename = Path(f\"measurements.txt\")  # Choose where to write to\n",
    "filename.unlink() if filename.exists() else None  # Delete the file if it exists already"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4032b-027c-4a9b-a3b0-b8a345b646d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run the data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0285ee3d-9b1c-4dfd-a738-691483fe18a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1 billion rows to measurements.txt: 100% in 25s (0s remaining)\n",
      "CPU times: user 10.1 s, sys: 18 s, total: 28.2 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop over chunks and generate data\n",
    "start = time.time()\n",
    "for i in range(int(n / chunksize)):\n",
    "    # Generate a chunk\n",
    "    generate_chunk(filename, chunksize, std, lookup_df)\n",
    "\n",
    "    # Update the progress bar\n",
    "    percent_complete = int(((i + 1) * chunksize) / n * 100)\n",
    "    time_taken = int(time.time() - start)\n",
    "    time_remaining = int((time_taken / percent_complete) * 100) - time_taken\n",
    "    print(\n",
    "        f\"Writing {int(n / 1e9)} billion rows to {filename}: {percent_complete}% in {time_taken}s ({time_remaining}s remaining)\",\n",
    "        end=\"\\r\",\n",
    "    )\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f139a-ec84-4d80-a73b-2ac345cb4ac3",
   "metadata": {},
   "source": [
    "If you watch the graphs while this cell is running you should see a burts of GPU utilization when the GPU generates the random numbers followed by a burst of Disk IO when that data is written to disk. This pattern will happen for each chunk that is generated.\n",
    "\n",
    "```{note}\n",
    "We could improve performance even further here by generating the next chunk while the current chunk is writing to disk, but a 30x speedup seems optimal enough for now.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82f9b0-61f4-4e8c-8312-d97638bd25cf",
   "metadata": {},
   "source": [
    "#### Check the files\n",
    "\n",
    "Now we can verify our dataset is the size we expected and contains rows that follow the format needed by the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e92fc1-696c-4593-a1df-9490c15d44ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 rapids conda 13G Jan 22 16:54 measurements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4a3e4c-5670-4c6e-91ef-863288aecdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guatemala City;17.3\n",
      "Launceston;24.3\n",
      "Bulawayo;8.7\n",
      "Tbilisi;9.5\n",
      "Napoli;26.8\n",
      "Sarajevo;27.5\n",
      "Chihuahua;29.2\n",
      "Ho Chi Minh City;8.4\n",
      "Johannesburg;19.2\n",
      "Cape Town;16.3\n"
     ]
    }
   ],
   "source": [
    "!head {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc7676",
   "metadata": {},
   "source": [
    "## GPU solution with RAPIDS\n",
    "\n",
    "Now let's look at using RAPIDS to speed up our Pandas implementation of the challenge. If you directly convert the reference implementation from Pandas to cuDF you will run into some [limitations cuDF has with string columns](https://github.com/rapidsai/cudf/issues/13733). Also depending on your GPU you may run into memory limits as cuDF will read the whole dataset into memory and machines typically have less GPU memory than CPU memory.\n",
    "\n",
    "Therefore to solve this with RAPIDS we also need to use [Dask](https://dask.org) to partition the dataset and stream it through GPU memory, then cuDF can process each partition in a performant way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750593c9",
   "metadata": {},
   "source": [
    "### Deploying Dask\n",
    "\n",
    "We are going to use [dask-cuda](/tools/dask-cuda/) to start a GPU Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e48b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "client = Client(LocalCUDACluster())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc25a17",
   "metadata": {},
   "source": [
    "Creating a `LocalCUDACluster()` inspects the machine and starts one Dask worker for each detected GPU. We then pass that to a Dask client which means that all following code in the notebook will leverage the GPU workers.\n",
    "\n",
    "```{tip}\n",
    "Dask has [a lot of different tools for deploying clusters](https://docs.dask.org/en/latest/ecosystem.html#deploying-dask), and they all follow the same format of instantiating a class. So whether you are trying to leverage all of the resources in a single machine like this example or trying to leverage an entire multi-node cluster Dask can get you up and running quickly.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24acf4d9-c5dc-42c5-8f73-92026f0cc581",
   "metadata": {},
   "source": [
    "### Dask dashboard\n",
    "\n",
    "We can also make use of the [Dask Dashboard](https://docs.dask.org/en/latest/dashboard.html) to see what is going on. \n",
    "\n",
    "If you select the Dask logo from the left-hand toolbar and then click the search icon it should detect our `LocalCUDACluster` automatically and show us a long list of graphs to choose from.\n",
    "\n",
    "![](./dask-labextension-graphs.png)\n",
    "\n",
    "When working with GPUs the \"GPU Utilization\" and \"GPU Memory\" will show us the same as the nvdashboard plots but for all machines in our Dask cluster. This is very helpful when working on a multi-node cluster but doesn't help us in thie single-node configuration.\n",
    "\n",
    "To see what Dask is doing in this challenge you should open the \"Progress\" and \"Task Stream\" graphs which will show all of the operations being performed. But feel free to open other graphs and explore all of the different metrics Dask can give you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f02767",
   "metadata": {},
   "source": [
    "### Dask + cuDF Solution\n",
    "\n",
    "Now that we have our input data and a Dask cluster we can write some Dask code that leverages cuDF under the hood to perform the compute operations.\n",
    "\n",
    "First we need to import `dask.dataframe` and tell it to use the `cudf` backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc71c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7fbd773ae590>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a792b2",
   "metadata": {},
   "source": [
    "Now we can run our Dask code, which is almost identical to the Pandas code we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eff3cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.59 s ± 124 ms per loop (mean ± std. dev. of 4 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 4\n",
    "df = dd.read_csv(\"measurements.txt\", sep=\";\", header=None, names=[\"station\", \"measure\"])\n",
    "df = df.groupby(\"station\").agg([\"min\", \"max\", \"mean\"])\n",
    "df.columns = df.columns.droplevel()\n",
    "\n",
    "# We need to switch back to Pandas for the final sort at the time of writing due to rapidsai/cudf#14794\n",
    "df = df.compute().to_pandas()\n",
    "df = df.sort_values(\"station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b0979",
   "metadata": {},
   "source": [
    "Running this notebook on a desktop workstation with two NVIDIA RTX 8000 GPUs completes the challenge in around **4 seconds** (a **60x speedup** over Pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb907d7-b386-46c7-bc41-1d31928cd054",
   "metadata": {},
   "source": [
    "Watching the progress bars you should see them fill and reset a total of 12 times as our `%%timeit` operation is solving the challenge multiple times to get an average speed.\n",
    "\n",
    "![](./dask-labextension-processing.png)\n",
    "\n",
    "In the above screenshot you can see that on a dual-GPU system Dask was leveraging both GPUs. But it's also interesting to note that the GPU utilization never reaches 100%. This is because the SSD in the machine has now become the bottleneck. The GPUs are performing the calculations so efficiently that we can't read data from disk fast enough to fully saturate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6cddb2-3e08-44a2-b054-2c153fd636a6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "RAPIDS can accelerate existing workflows written with libraries like Pandas with little to no code changes. GPUs can accelerate computations by orders of magnitude which can move performance bottlenecks to other parts of the system.\n",
    "\n",
    "Using dashboarding tools like nvdashboard and the Dask dashboard allow you to see and understand how your system is performing. Perhaps in this example upgrading the SSD is the next step to achieving even more performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
