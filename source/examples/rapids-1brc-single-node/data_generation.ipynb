{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78bfe48c-47f8-4407-821f-ba1efdcfd9da",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "Based on a lookup table of cities and their mean temperatures we want to generate a CSV file containing `n` rows of random temperatures.\n",
    "\n",
    "To generate each row we choose a random city from the lookup table, then generate a random temperature from a normal distribution around the mean temp. We assume the standard deviation is `10.0` for all cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68dcc86b-ddc1-4283-a129-7f33870d4c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import cudf\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d86b4a-62ed-45e8-9202-8ee4b45f2aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_chunk(filename, chunksize, std, lookup_df):\n",
    "    \"\"\"Generate some sample data based on the lookup table.\"\"\"\n",
    "    # Generate a normal distibution around zero for each row in our output, we will fix the mean for each one in a minute\n",
    "    temps = cp.random.normal(0, std, int(chunksize))\n",
    "    # Choose a random city from the lookup table for each row in our output\n",
    "    cities = cp.random.random_integers(0, len(lookup_df) - 1, int(chunksize))\n",
    "\n",
    "    @cuda.jit\n",
    "    def offset_kernel(temps, cities, lookup_values):\n",
    "        \"\"\"Lookup the mean of each city and offset the random temperature by the mean\"\"\"\n",
    "        i = cuda.grid(1)\n",
    "        if i < len(temps):\n",
    "            temps[i] = temps[i] + lookup_values[cities[i]]\n",
    "\n",
    "    # Offset each city by it's mean value\n",
    "    offset_kernel[math.ceil(len(temps) / 128), 128](\n",
    "        temps, cities, lookup_df.mean_temp.values\n",
    "    )\n",
    "\n",
    "    # Convert our arrays to a Dataframe\n",
    "    output_df = cudf.DataFrame({\"city\": cities, \"temp\": temps})\n",
    "    # Convert the random city index to the city name\n",
    "    output_df.city = output_df.city.astype(int).map(lookup_df.city)\n",
    "    # Round the temprature to one decimal place\n",
    "    output_df.temp = output_df.temp.round(decimals=1)\n",
    "    # Append this chunk to the output file\n",
    "    with open(filename, \"a\") as fh:\n",
    "        output_df.to_csv(fh, sep=\";\", chunksize=10_000_000, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1d27-f890-448a-8841-3cb4547a0a9b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71484827-cf06-42b8-b635-c95edf0c3380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 1_000_000_000  # Number of rows of data to generate\n",
    "\n",
    "lookup_df = cudf.read_csv(\n",
    "    \"lookup.csv\"\n",
    ")  # Load our lookup table of cities and their mean temperatures\n",
    "std = 10.0  # We assume temperatures are normally distributed with a standard deviation of 10\n",
    "chunksize = (\n",
    "    2e8  # Number of rows to generate in one go (tweak this based on your GPU RAM)\n",
    ")\n",
    "filename = Path(f\"data_{int(n / 1e9)}b.txt\")  # Choose where to write to\n",
    "if filename.exists():\n",
    "    filename.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4032b-027c-4a9b-a3b0-b8a345b646d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0285ee3d-9b1c-4dfd-a738-691483fe18a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1 billion rows to data_1b.txt: 100% in 23s (0s remaining)\n",
      "CPU times: user 8.43 s, sys: 15.7 s, total: 24.2 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop over chunks and generate data\n",
    "start = time.time()\n",
    "for i in range(int(n / chunksize)):\n",
    "    generate_chunk(filename, chunksize, std, lookup_df)\n",
    "    percent_complete = int(((i + 1) * chunksize) / n * 100)\n",
    "    time_taken = int(time.time() - start)\n",
    "    time_remaining = int((time_taken / percent_complete) * 100) - time_taken\n",
    "    print(\n",
    "        f\"Writing {int(n / 1e9)} billion rows to {filename}: {percent_complete}% in {time_taken}s ({time_remaining}s remaining)\",\n",
    "        end=\"\\r\",\n",
    "    )\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82f9b0-61f4-4e8c-8312-d97638bd25cf",
   "metadata": {},
   "source": [
    "## Check the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e92fc1-696c-4593-a1df-9490c15d44ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 rapids conda 129G Jan 18 14:55 data_10b.txt\n",
      "-rw-r--r-- 1 rapids conda  13G Jan 18 14:41 data_1b.txt\n",
      "-rw-r--r-- 1 rapids conda  26G Jan 18 14:18 data_2b.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data_*.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
