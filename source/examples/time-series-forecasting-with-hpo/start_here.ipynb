{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671dd603-6b51-46b2-98b3-2b05c7c92c38",
   "metadata": {},
   "source": [
    "# Perform time series forecasting on Google Kubernetes Engine with NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18814f8-5374-468f-9877-28275dbf20d6",
   "metadata": {},
   "source": [
    "In this example, we will be looking at a real-world example of **time series forecasting** with data from [the M5 Forecasting Competition](https://www.kaggle.com/competitions/m5-forecasting-accuracy). Walmart provides historical sales data from multiple stores in three states, and our job is to predict the sales in a future 28-day period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8dc16-70a9-41cc-b846-88e83a2aa660",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168f224-8549-41a7-a6a7-2023c83bb466",
   "metadata": {},
   "source": [
    "### Prepare GKE cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde94ab8-123c-4a72-95db-86a2098247bb",
   "metadata": {},
   "source": [
    "To run the example, you will need a working Google Kubernetes Engine (GKE) cluster with access to NVIDIA GPUs. Use the following resources to set up a cluster:\n",
    "\n",
    "* [Set up a GKE cluster with access to NVIDIA GPUs](https://docs.rapids.ai/deployment/stable/cloud/gcp/gke/)\n",
    "* [Install the Dask-Kubernetes operator](https://kubernetes.dask.org/en/latest/operator_installation.html)\n",
    "* [Install Kubeflow](https://www.kubeflow.org/docs/started/installing-kubeflow/)\n",
    "\n",
    "Kubeflow is not strictly necessary, but we highly recommend it, as Kubeflow gives you a nice notebook environment to run this notebook within the k8s cluster. (You may choose any method; we tested this example after installing Kubeflow from manifests.) When creating the notebook environment, use the following configuration:\n",
    "\n",
    "* 2 CPUs, 16 GiB of memory\n",
    "* 1 NVIDIA GPU\n",
    "* 40 GiB disk volume\n",
    "\n",
    "After uploading all the notebooks in the example, run this notebook (`start_here.ipynb`) in the notebook environment.\n",
    "\n",
    "Note: We will use the worker pods to speed up the training stage. The preprocessing steps will run solely on the scheduler node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b63586-119e-47ee-ba74-063cfca71fe0",
   "metadata": {},
   "source": [
    "### Prepare a bucket in Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4ada1-60c5-4456-b82d-953ce499a2fa",
   "metadata": {},
   "source": [
    "Create a new bucket in Google Cloud Storage. Make sure that the worker pods in the k8s cluster has read/write access to this bucket. This can be done in one of the following methods:\n",
    "\n",
    "1. Option 1: Specify an additional scope when provisioning the GKE cluster.\n",
    "\n",
    "   When you are provisioning a new GKE cluster, add the `storage-rw` scope.\n",
    "   This option is only available if you are creating a new cluster from scratch. If you are using an exising GKE cluster, see Option 2.\n",
    "\n",
    "   Example:\n",
    "```\n",
    "gcloud container clusters create my_new_cluster --accelerator type=nvidia-tesla-t4 \\\n",
    "   --machine-type n1-standard-32 --zone us-central1-c --release-channel stable \\\n",
    "   --num-nodes 5 --scopes=gke-default,storage-rw\n",
    "```\n",
    "\n",
    "2. Option 2: Grant bucket access to the associated service account.\n",
    "\n",
    "   Find out which service account is associated with your GKE cluster. You can grant the bucket access to the service account as follows: Nagivate to the Cloud Storage console, open the Bucket Details page for the bucket, open the Permissions tab, and click on Grant Access.\n",
    "   \n",
    "Enter the name of the bucket that your cluster has read-write access to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45bc19b6-f5f9-4f55-827b-db9484c125ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = \"<Put the name of the bucket here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704898b3-e2b0-4b40-bcd3-178a0bdeee6f",
   "metadata": {},
   "source": [
    "### Install Python packages in the notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf7b111-3aba-4fae-b805-fb3063d5a621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install kaggle gcsfs dask-kubernetes optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13737fe2-3df5-4614-8675-fb20bccf7a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the bucket is accessible\n",
    "import gcsfs\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "fs.ls(f\"{bucket_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2274c-90a0-484f-ad71-33ddba170f09",
   "metadata": {},
   "source": [
    "## Obtain the time series data set from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cde5ee-dad0-4ee2-9c26-7a36b6bcee49",
   "metadata": {},
   "source": [
    "If you do not yet have an account with Kaggle, create one now. Then follow instructions in [Public API Documentation of Kaggle](https://www.kaggle.com/docs/api) to obtain the API key. This step is needed to obtain the training data from the M5 Forecasting Competition. Once you obtained the API key, fill in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2eb62a-a0f5-4801-9254-367fcef05e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kaggle_username = \"<Put your Kaggle username here>\"\n",
    "kaggle_api_key = \"<Put your Kaggle API key here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd487f-0de1-4c41-ba02-7189493739f3",
   "metadata": {},
   "source": [
    "Now we are ready to download the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c6fd3fe-b2d3-4bff-88f5-b86de68878a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env KAGGLE_USERNAME=$kaggle_username\n",
    "%env KAGGLE_KEY=$kaggle_api_key\n",
    "\n",
    "!kaggle competitions download -c m5-forecasting-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7141384-e825-4387-a9f5-42b932e27e65",
   "metadata": {},
   "source": [
    "Let's unzip the ZIP archive and see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7bf2248-cd3b-47c5-8923-ec9a6e868a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  m5-forecasting-accuracy.zip\n",
      "  inflating: data/calendar.csv       \n",
      "  inflating: data/sales_train_evaluation.csv  \n",
      "  inflating: data/sales_train_validation.csv  \n",
      "  inflating: data/sample_submission.csv  \n",
      "  inflating: data/sell_prices.csv    \n"
     ]
    }
   ],
   "source": [
    "!unzip m5-forecasting-accuracy.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "673468ae-9f6d-499f-86c4-230021bbf1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root users 102K Jun  1  2020 data/calendar.csv\n",
      "-rw-r--r-- 1 root users 117M Jun  1  2020 data/sales_train_evaluation.csv\n",
      "-rw-r--r-- 1 root users 115M Jun  1  2020 data/sales_train_validation.csv\n",
      "-rw-r--r-- 1 root users 5.0M Jun  1  2020 data/sample_submission.csv\n",
      "-rw-r--r-- 1 root users 194M Jun  1  2020 data/sell_prices.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304ea68-381f-45b4-9e27-201a35e31239",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9903e47-2a83-40d1-b65b-d5818e9f0647",
   "metadata": {},
   "source": [
    "We are now ready to run the preprocessing steps. You should run the six notebooks in order, to process the raw data into a form that can be used for model training:\n",
    "\n",
    "* `preprocessing_part1.ipynb`\n",
    "* `preprocessing_part2.ipynb`\n",
    "* `preprocessing_part3.ipynb`\n",
    "* `preprocessing_part4.ipynb`\n",
    "* `preprocessing_part5.ipynb`\n",
    "* `preprocessing_part6.ipynb`\n",
    "* `training_and_evaluation.ipynb`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
