{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "platform/kubernetes",
     "cloud/gcp/gke",
     "tools/dask-operator",
     "library/cuspatial",
     "library/dask",
     "library/cudf",
     "data-format/parquet",
     "data-storage/gcs"
    ]
   },
   "source": [
    "# Autoscaling multi-tenant Kubernetes Deep-Dive\n",
    "\n",
    "In this example we are going to take a deep-dive into launching an autoscaling multi-tenant RAPIDS environment on Kubernetes.\n",
    "\n",
    "Being able to scale out your workloads and only pay for the resources you use is a fantastic way to save costs when using RAPIDS. If you have many folks in your organization who all want to be able to do this you can get added benefits by pooling your resources into an autoscaling Kubernetes cluster.\n",
    "\n",
    "Let's run through the steps required to launch a Kubernetes cluster on [Google Cloud](https://cloud.google.com), then simulate the workloads of many users sharing the cluster. Then we can explore what that experience was like both from a user perspective and also from a cost perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before we get started you'll need to ensure you have a few CLI tools installed.\n",
    "\n",
    "- [`gcloud`](https://cloud.google.com/sdk/gcloud) (and make sure you run [`gcloud auth login`](https://cloud.google.com/sdk/gcloud/reference/auth/login))\n",
    "- [`kubectl`](https://kubernetes.io/docs/tasks/tools/)\n",
    "- [`helm`](https://helm.sh/docs/intro/install/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get a Kubernetes Cluster\n",
    "\n",
    "For this example we are going to use [Google Cloud's Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine) to launch a cluster.\n",
    "\n",
    "````{docref} /cloud/gcp/gke\n",
    "We are going to follow the RAPIDS GKE deployment instructions but we will modify our cluster creation command to enable Kubernetes cluster autoscaling out of the box.\n",
    "\n",
    "```\n",
    "--enable-autoscaling --autoscaling-profile optimize-utilization \\\n",
    "--num-nodes 1 --min-nodes 1 --max-nodes 20\n",
    "```\n",
    "\n",
    "Data science container images are also notiriously large so we will enable image streaming to speed up our container creation.\n",
    "\n",
    "```\n",
    "--image-type=\"COS_CONTAINERD\" --enable-image-streaming\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud container clusters create multi-tenant-rapids-k8 \\\n",
    "    --accelerator type=nvidia-tesla-t4,count=2 --machine-type n1-standard-4 \\\n",
    "    --region us-central1 --node-locations us-central1-b,us-central1-c \\\n",
    "    --release-channel stable \\\n",
    "    --enable-autoscaling --autoscaling-profile optimize-utilization \\\n",
    "    --num-nodes 1 --min-nodes 1 --max-nodes 20 \\\n",
    "    --image-type=\"COS_CONTAINERD\" --enable-image-streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our cluster let's [install the NVIDIA Drivers](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Observability\n",
    "\n",
    "Once we have run some workloads on our Kubernetes cluster we will want to be able to go back through the cluster telemetry data to see how our autoscaling behaved. To do this let's install [Prometheus](https://prometheus.io/) so that we are recording cluster metrics and can explore them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prometheus stack\n",
    "\n",
    "Let's start by installing the [Kubernetes Prometheus Stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) which includes everything we need to run Prometheus on our cluster.\n",
    "\n",
    "We need to add a couple of extra configuration options to ensure Prometheus is collecting data frequently enough to analyse, which you will find in `prometheus-stack-values.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat prometheus-stack-values.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! helm install --repo https://prometheus-community.github.io/helm-charts kube-prometheus-stack kube-prometheus-stack \\\n",
    "   --create-namespace --namespace prometheus \\\n",
    "   --values prometheus-stack-values.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Prometheus running and collecting data we can move on and install RAPIDS and run some workloads. We will come back to these tools later when we want to explore the data we have collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install RAPIDS\n",
    "\n",
    "For this RAPIDS installation we are going to use a single [Jupyter Notebook Pod](/platforms/kubernetes) and the [Dask Operator](/tools/kubernetes/dask-operator). In a real deployment you would use something like [JupyterHub](https://jupyter.org/hub) or [Kubeflow Notebooks](https://www.kubeflow.org/docs/components/notebooks/) to create a notebook spawning service with user authentication, but that is out of scope for this example.\n",
    "\n",
    "```{docref} /platforms/kubernetes\n",
    "There are many ways to install RAPIDS on Kubernetes. You can find detailed instructions on all of the various methods in the documentation.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image steaming (optional)\n",
    "\n",
    "In order to steam the container image to the GKE nodes our image needs to be stored in [Google Cloud Artifact Registry](https://cloud.google.com/artifact-registry/) in the same region as our cluster.\n",
    "\n",
    "```console\n",
    "$ docker pull \"{{ rapids_container }}\"\n",
    "\n",
    "$ docker tag \"{{ rapids_container }}\" REGION-docker.pkg.dev/PROJECT/REPO/IMAGE:TAG\n",
    "\n",
    "$ docker push REGION-docker.pkg.dev/PROJECT/REPO/IMAGE:TAG\n",
    "```\n",
    "\n",
    "Be sure to replace the image throughout the notebook with the one that you have pushed to your own Google Cloud project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapids_image = \"{{ rapids_container }}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull rapids_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag rapids_image us-central1-docker.pkg.dev/nv-ai-infra/rapidsai/rapidsai/base:23.08-cuda12.0-py3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push us-central1-docker.pkg.dev/nv-ai-infra/rapidsai/rapidsai/base:23.08-cuda12.0-py3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Image prepuller (optional)\n",
    "\n",
    "If you know that many users are going to want to frequently pull a specific container image I like to run a small `DaemonSet` which ensures that image starts streaming onto a node as soon as it joins the cluster. This is optional but can reduce wait time for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat image-prepuller.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl apply -f image-prepuller.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RAPIDS Notebook Pod\n",
    "\n",
    "Now let's launch a Notebook Pod. \n",
    "\n",
    "````{note}\n",
    "From this Pod we are going to want to be able to spawn Dask cluster resources on Kubernetes, so we need to ensure the Pod has the appropriate permissions to interact with the Kubernetes API. \n",
    "\n",
    "```{docref} /platforms/kubernetes\n",
    "Check out the extended notebook configuration documentation for more details.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl apply -f rapids-notebook.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install the Dask Operator\n",
    "\n",
    "Lastly we need to install the Dask Operator so we can spawn RAPIDS Dask cluster from our Notebook session.\n",
    "\n",
    "```{docref} /tools/kubernetes/dask-operator\n",
    "See the RAPIDS Dask Operator documentation for more information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! helm install --repo https://helm.dask.org dask-kubernetes-operator \\\n",
    "    --generate-name --create-namespace --namespace dask-operator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some work\n",
    "\n",
    "Next let's connect to the Jupyter session and run some work on our cluster. You can do this by port forwarding the Jupyter service to your local machine.\n",
    "\n",
    "```console\n",
    "$ kubectl port-forward svc/rapids-notebook 8888:8888                                                                                                                        \n",
    "Forwarding from 127.0.0.1:8888 -> 8888\n",
    "Forwarding from [::1]:8888 -> 8888\n",
    "```\n",
    "\n",
    "Then open http://localhost:8888 in your browser.\n",
    "\n",
    "```{note}\n",
    "If you are following along with this notebook locally you will also want to upload it to the Jupyter session and continue running the cells from there.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check our capabilities\n",
    "\n",
    "Let's make sure our environment is all set up correctly by checking out our capabilities. We can start by running `nvidia-smi` to inspect our Notebook GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we can see our notebook has an NVIDIA T4. Now let's use `kubectl` to inspect our cluster. We won't actually have `kubectl` installed in our remote Jupyter environment so let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mamba install --quiet -c conda-forge kubernetes-client -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our prepull Pods we created earlier alongside our `rapids-notebook` Pod that we are currently in. As we created the prepull Pod via a `DaemonSet` we also know that there are two nodes in our Kubernetes cluster because there are two prepull Pods. As our cluster scales we will see more of them appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get daskclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that we currently have no `DaskCluster` resources, but this is good because we didn't get a `server doesn't have a resource type \"daskclusters\"` error so we know the Dask Operator also installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small workload\n",
    "\n",
    "Let's run a small RAPIDS workload that stretches our Kubernetes cluster a little and causes it to scale. \n",
    "\n",
    "We know that we have two nodes in our Kubernetes cluster and we selected a node type with 2 GPUs when we launched it on GKE. Our Notebook Pod is taking up one GPU so we have three remaining. If we launch a Dask Cluster we will need one GPU for the scheduler and one for each worker. So let's create a Dask cluster with four workers which will cause our Kubernetes to add one more node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's install `dask-kubernetes` so we can create our `DaskCluster` resources from Python. We will also install `gcsfs` so that our workload can read data from [Google Cloud Storage](https://cloud.google.com/storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mamba install --quiet -c conda-forge dask-kubernetes gcsfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes.operator import KubeCluster\n",
    "\n",
    "cluster = KubeCluster(\n",
    "    name=\"rapids-dask-1\",\n",
    "    image=rapids_image,  # Replace me with your cached image\n",
    "    n_workers=4,\n",
    "    resources={\"limits\": {\"nvidia.com/gpu\": \"1\"}},\n",
    "    env={\"EXTRA_PIP_PACKAGES\": \"gcsfs\"},\n",
    "    worker_command=\"dask-cuda-worker\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great our Dask cluster was created but right now we just have a scheduler with half of our workers. We can use `kubectl` to see what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that most of our Pods are `Running` but two workers are `Pending`. This is because we don't have enough GPUs for them right now. We can look at the events on our pending pods for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get event --field-selector involvedObject.name=rapids-dask-1-default-worker-5f59bc8e7a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our Pod triggered the cluster to scale from one to two nodes. If we wait for our new node to come online we should see a few things happen. \n",
    "\n",
    "- First there will be a new prepull Pod scheduled on the new node which will start streaming the RAPIDS container image.\n",
    "- Other Pods in the `kube-system` namespace will be scheduled to install NVIDIA drivers and update the Kubernetes API.\n",
    "- Then once the GPU drivers have finished installing the worker Pods will be scheduled onto our new node\n",
    "- Then once the image is ready our Pods move into a `Running` phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome we can now run some work on our Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some data from GCS into memory on our GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.config\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "\n",
    "df = dd.read_parquet(\n",
    "    \"gcs://anaconda-public-data/nyc-taxi/2015.parquet\",\n",
    "    storage_options={\"token\": \"cloud\"},\n",
    ").persist()\n",
    "wait(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some calculation. This can be whatever you want to do with your data, for this example let's do something quick like calculating the haversine distance between the pickup and dropoff locations (yes calculating this on ~100M rows is a quick task for RAPIDS 😁)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuspatial import haversine_distance\n",
    "\n",
    "\n",
    "def map_haversine(part):\n",
    "    return haversine_distance(\n",
    "        part[\"pickup_longitude\"],\n",
    "        part[\"pickup_latitude\"],\n",
    "        part[\"dropoff_longitude\"],\n",
    "        part[\"dropoff_latitude\"],\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"haversine_distance\"] = df.map_partitions(map_haversine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[\"haversine_distance\"].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we now have a little toy workloads that opens some data, does some calculation and takes a bit of time.\n",
    "\n",
    "Let's remove our single Dask cluster and switch to simulating many workloads running at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating many multi-tenant workloads\n",
    "\n",
    "Now we have a toy workload which we can use to represent one user on our multi-tenant cluster.\n",
    "\n",
    "Let's now construct a larger graph to simulate lots of users spinning up Dask clusters and running workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a function that contains our whole workload including our cluster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.delayed\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def run_haversine(*args):\n",
    "    from dask_kubernetes.operator import KubeCluster\n",
    "    from dask.distributed import Client, wait\n",
    "    import uuid\n",
    "    import dask.config\n",
    "    import dask.dataframe as dd\n",
    "\n",
    "    dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "\n",
    "    def map_haversine(part):\n",
    "        from cuspatial import haversine_distance\n",
    "\n",
    "        return haversine_distance(\n",
    "            part[\"pickup_longitude\"],\n",
    "            part[\"pickup_latitude\"],\n",
    "            part[\"dropoff_longitude\"],\n",
    "            part[\"dropoff_latitude\"],\n",
    "        )\n",
    "\n",
    "    with KubeCluster(\n",
    "        name=\"rapids-dask-\" + uuid.uuid4().hex[:5],\n",
    "        image=rapids_image,  # Replace me with your cached image\n",
    "        n_workers=2,\n",
    "        resources={\"limits\": {\"nvidia.com/gpu\": \"1\"}},\n",
    "        env={\"EXTRA_PIP_PACKAGES\": \"gcsfs\"},\n",
    "        worker_command=\"dask-cuda-worker\",\n",
    "        resource_timeout=600,\n",
    "    ) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            client.wait_for_workers(2)\n",
    "            df = dd.read_parquet(\n",
    "                \"gcs://anaconda-public-data/nyc-taxi/2015.parquet\",\n",
    "                storage_options={\"token\": \"cloud\"},\n",
    "            )\n",
    "            client.compute(df.map_partitions(map_haversine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we run this function we will launch a Dask cluster and run our workload. We will use context managers to ensure our Dask cluster gets cleaned up when the work is complete. Given that we have no active Dask clusters this function will be executed on the Notebook Pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_haversine().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great that works, so we have a self contained RAPIDS workload that launches its own Dask cluster and performs some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating our multi-tenant workloads\n",
    "\n",
    "To see how our Kubernetes cluster behaves when many users are sharing it we want to run our haversine workload a bunch of times. \n",
    "\n",
    "```{note}\n",
    "If you're not interested in how we simulate this workload feel free to skip onto the analysis section.\n",
    "```\n",
    "\n",
    "To do this we can create another Dask cluster which we will use to pilot our workloads. This cluster will be a proxy for the Jupyter sessions our users would be interacting with. Then we will construct a Dask graph which runs our haversine workload many times in various configurations to simulate different users submitting different workloads on an ad-hoc basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes.operator import KubeCluster, make_cluster_spec\n",
    "\n",
    "cluster_spec = make_cluster_spec(\n",
    "    name=\"mock-jupyter-cluster\",\n",
    "    image=rapids_image,  # Replace me with your cached image\n",
    "    n_workers=1,\n",
    "    resources={\"limits\": {\"nvidia.com/gpu\": \"1\"}, \"requests\": {\"cpu\": \"50m\"}},\n",
    "    env={\"EXTRA_PIP_PACKAGES\": \"gcsfs dask-kubernetes\"},\n",
    ")\n",
    "cluster_spec[\"spec\"][\"worker\"][\"spec\"][\"serviceAccountName\"] = \"rapids-dask\"\n",
    "\n",
    "cluster = KubeCluster(custom_cluster_spec=cluster_spec)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure our workers have the same dependencies as our Notebook session here so that it can spawn more Dask clusters so we install `gcsfs` and `dask-kubernetes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets submit our workload again but this time to our cluster. Our function will be sent to our \"Jupyter\" worker which will then spawn another Dask cluster to run the workload. We don't have enough GPUs in our cluster to do this so it will trigger another scale operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_haversine().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a small function which we can use to build up arbitrarily complex workloads. We can define how many stages we have, how many concurrent Dask clusters their should be, how quickly to vary width over time, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "def generate_workload(\n",
    "    stages=3, min_width=1, max_width=3, variation=1, input_workload=None\n",
    "):\n",
    "    graph = [input_workload] if input_workload is not None else [run_haversine()]\n",
    "    last_width = min_width\n",
    "    for stage in range(stages):\n",
    "        width = randrange(\n",
    "            max(min_width, last_width - variation),\n",
    "            min(max_width, last_width + variation) + 1,\n",
    "        )\n",
    "        graph = [run_haversine(*graph) for _ in range(width)]\n",
    "        last_width = width\n",
    "    return run_haversine(*graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(3)  # Let's also bump up our user cluster to show more users logging in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize our graphs let's check that we have `graphviz` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge --quiet graphviz python-graphviz -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a small workload which will run a couple of stages and trigger a scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workload = generate_workload(stages=2, max_width=2)\n",
    "workload.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great we have multiple stages where one or two users are running workloads at the same time. Now lets chain a bunch of these workloads together to simulate varying demands over a larger period of time.\n",
    "\n",
    "We will also track the start and end times of the run so that we can grab the right data from Prometheus later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "The next cell will take around 1h to run.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "start_time = (datetime.datetime.now() - datetime.timedelta(minutes=15)).strftime(\n",
    "    \"%Y-%m-%dT%H:%M:%SZ\"\n",
    ")\n",
    "try:\n",
    "    # Start with a couple of concurrent workloads\n",
    "    workload = generate_workload(stages=10, max_width=2)\n",
    "    # Then increase demand as more users appear\n",
    "    workload = generate_workload(\n",
    "        stages=5, max_width=5, min_width=3, variation=5, input_workload=workload\n",
    "    )\n",
    "    # Now reduce the workload for a longer period of time, this could be over a lunchbreak or something\n",
    "    workload = generate_workload(stages=30, max_width=2, input_workload=workload)\n",
    "    # Everyone is back from lunch and it hitting the cluster hard\n",
    "    workload = generate_workload(\n",
    "        stages=10, max_width=10, min_width=3, variation=5, input_workload=workload\n",
    "    )\n",
    "    # The after lunch rush is easing\n",
    "    workload = generate_workload(\n",
    "        stages=5, max_width=5, min_width=3, variation=5, input_workload=workload\n",
    "    )\n",
    "    # As we get towards the end of the day demand slows off again\n",
    "    workload = generate_workload(stages=10, max_width=2, input_workload=workload)\n",
    "    workload.compute()\n",
    "finally:\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    end_time = (datetime.datetime.now() + datetime.timedelta(minutes=15)).strftime(\n",
    "        \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great, our large graph of workloads resulted in ~200 clusters launching throughout the run with varying capacity demands and took just over an hour to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's explore the data we've been collecting with Prometheus to see how our cluster perforumed during our simulated workload. We could do this in [Grafana](https://grafana.com/), but instead let's stay in the notebook and use `prometheus-pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install prometheus-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the prometheus endpoint within our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_pandas import query\n",
    "\n",
    "p = query.Prometheus(\"http://kube-prometheus-stack-prometheus.prometheus:9090\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pending pods\n",
    "\n",
    "First let's see how long each of our Pods spent in a `Pending` phase. This is the amount of time users would have to wait for their work to start running when they create their Dask clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_pods = p.query_range(\n",
    "    'kube_pod_status_phase{phase=\"Pending\",namespace=\"default\"}',\n",
    "    start_time,\n",
    "    end_time,\n",
    "    \"1s\",\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.utils import format_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average time for Pod creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_time(pending_pods.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_time(pending_pods.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99th percentile time for Pod creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_time(pending_pods.quantile(0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers seem great, the most common start time for a cluster is two seconds! With the average being around 20 seconds. If your cluster triggers Kubernetes to scale up you could be waiting for 5 minutes though. Let's see how many users would end up in that situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of users get workers in less than 2 seconds, 5 seconds, 60 seconds, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.percentileofscore(pending_pods, 2.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.percentileofscore(pending_pods, 5.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.percentileofscore(pending_pods, 60.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok this looks pretty reasonable. Nearly 75% of users get a cluster in less than 5 seconds, and over 90% get it in under a minute. But if you're in the other 10% you may have to wait for 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bucket this data to see the distribution of startup times visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pending_pods.hist(bins=range(0, 600, 30))\n",
    "ax.set_title(\"Dask Worker Pod wait times\")\n",
    "ax.set_xlabel(\"Seconds\")\n",
    "ax.set_ylabel(\"Pods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pending_pods.hist(bins=range(0, 60, 2))\n",
    "ax.set_title(\"Dask Worker Pod wait times (First minute)\")\n",
    "ax.set_xlabel(\"Seconds\")\n",
    "ax.set_ylabel(\"Pods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see clearly that most users get their worker Pods scheduled in less than 5 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster scaling and efficiency\n",
    "\n",
    "Ok so our users are getting clusters nice and quick, that's because there is some warm capacity in the Kubernetes cluster that they are able to grab. When the limit is reached GKE autoscales to add new nodes. When demand drops for a while capacity is released again to save cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query to see how many nodes there were during the run and combine that with the number of running GPU Pods there were to see how efficiently we were using our resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_pods = p.query_range(\n",
    "    'kube_pod_status_phase{phase=~\"Running|ContainerCreating\",namespace=\"default\"}',\n",
    "    start_time,\n",
    "    end_time,\n",
    "    \"1s\",\n",
    ")\n",
    "running_pods = running_pods[\n",
    "    running_pods.columns.drop(list(running_pods.filter(regex=\"prepull\")))\n",
    "]\n",
    "nodes = p.query_range(\"count(kube_node_info)\", start_time, end_time, \"1s\")\n",
    "nodes.columns = [\"Available GPUs\"]\n",
    "nodes[\"Available GPUs\"] = (\n",
    "    nodes[\"Available GPUs\"] * 2\n",
    ")  # We know our nodes each had 2 GPUs\n",
    "nodes[\"Utilized GPUs\"] = running_pods.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent so we can see our cluster adding and removing nodes as our workload demand changed. The space between the orange and blue lines is our warm capacity. Ideally we want this to be as small as possible. Let's calculate what the gap is.\n",
    "\n",
    "How many GPU hours did our users utilize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_hours_utilized = nodes[\"Utilized GPUs\"].sum() / 60 / 60\n",
    "gpu_hours_utilized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many GPU hours were we charged for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_hours_cost = nodes[\"Available GPUs\"].sum() / 60 / 60\n",
    "gpu_hours_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the overhead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overhead = (1 - (gpu_hours_utilized / gpu_hours_cost)) * 100\n",
    "str(int(overhead)) + \"% overhead\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok not bad, so on our interactive cluster we managed 64% utilization of our GPU resources. Compared to non-autoscaling workloads where users interactively use long running workstations and clusters this is fantastic.\n",
    "\n",
    "If we measured batch workloads that ran for longer periods we would see this utilization clumb much higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing thoughts\n",
    "\n",
    "By sharing a Kubernetes cluster between many users who are all launching many ephemeral Dask Clusters to perform their work we are able to balance cost vs user time. Peaks in individual user demands get smoothed out over time in a multi-tenant model, and the overall peaks and troughs of the day are accomodated by the Kubernetes cluster autoscaler.\n",
    "\n",
    "We managed to create a responsive experience for our users where they generally got Dask clusters in a few seconds. We also managed to hit 64% utilization of the GPUs in our cluster, a very respectable number for an interactive cluster. \n",
    "\n",
    "There are more things we could tune to increase utilization, but there are also some tradeoffs to be made here. If we scale down more aggressively then we would end up needing to scale back up more often resulting in more users waiting longer for their clusters. \n",
    "\n",
    "We can also see there there is some unused capacity between the nodes starting and our workload running. This is the time when image pulling happens, drivers get installed, etc. There are definitely things we could do to improve this so that nodes are ready to go as soon as they have booted.\n",
    "\n",
    "Compared to every user spinning up dedicated nodes for their individual workloads and paying the driver install and environment pull wait time and overhead cost every time, we are pooling our resources and reusing our capacity effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teardown\n",
    "\n",
    "Finally to clean everything up we can delete our GKE cluster by running the following command locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud container clusters delete multi-tenant-rapids --region us-central1 --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.08",
   "language": "python",
   "name": "rapids-23.08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
