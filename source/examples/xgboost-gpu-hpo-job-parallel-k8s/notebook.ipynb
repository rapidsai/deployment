{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45684b2f-019b-4ccf-af65-b722aa270e40",
   "metadata": {
    "tags": [
     "library/xgboost",
     "library/optuna",
     "library/dask",
     "tools/dask-kubernetes",
     "platform/kubernetes",
     "platform/kubeflow",
     "library/scikit-learn",
     "workflow/hpo"
    ]
   },
   "source": [
    "# Scaling up hyperparameter optimization with Kubernetes and XGBoost GPU algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca704d4",
   "metadata": {},
   "source": [
    "Choosing an optimal set of hyperparameters is a daunting task, especially for algorithms like XGBoost that have many hyperparameters to tune. In this notebook, we will show how to speed up hyperparameter optimization by running multiple training jobs in parallel on a Kubernetes cluster.\n",
    "\n",
    "# Prerequisites\n",
    "Please follow instructions in [Dask Operator: Installation](../../tools/kubernetes/dask-operator.md) to install the Dask operator on top of a GPU-enabled Kubernetes cluster. (For the purpose of this example, you may ignore other sections of the linked document.)\n",
    "\n",
    "## Optional: Kubeflow\n",
    "Kubeflow gives you a nice notebook environment to run this notebook within the k8s cluster. Install Kubeflow by following instructions in [Installing Kubeflow](https://www.kubeflow.org/docs/started/installing-kubeflow/). You may choose any method; we tested this example after installing Kubeflow from manifests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0c861-9357-48b1-a317-a9df3deb2319",
   "metadata": {},
   "source": [
    "# Install system packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3177609-0300-461b-a961-8b3a62943e9e",
   "metadata": {},
   "source": [
    "We'll need extra Python packages. In particular, we need an unreleased version of Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb24c4-4358-49a5-9770-88a49bc27689",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install dask_kubernetes optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a35c4d-07f6-4e42-b865-dbd2e54ed369",
   "metadata": {},
   "source": [
    "# Set up Dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6a7d9-b943-43e4-b1ed-c6110412ce3f",
   "metadata": {},
   "source": [
    "Let us set up a Dask cluster using the `KubeCluster` class. Fill in the following variables, depending on the configuration of your Kubernetes cluster. Here how you can get `n_workers`, assuming that you are using all the nodes in the Kubernetes cluster. Let `N` be the number of nodes.\n",
    "* On AWS Elastic Kubernetes Service (EKS): `n_workers = N - 2`\n",
    "* On Google Cloud Kubernetes: `n_workers = N - 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f63a9-3848-4804-8a1a-3cee5dcbe756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the same RAPIDS image you used for launching the notebook session\n",
    "rapids_image = \"{{ rapids_container }}\"\n",
    "# Use the number of worker nodes in your Kubernetes cluster.\n",
    "n_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad26b9-3280-4087-b945-1cb7f9fb0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes.operator import KubeCluster\n",
    "\n",
    "cluster = KubeCluster(\n",
    "    name=\"rapids-dask\",\n",
    "    image=rapids_image,\n",
    "    worker_command=\"dask-cuda-worker\",\n",
    "    n_workers=n_workers,\n",
    "    resources={\"limits\": {\"nvidia.com/gpu\": \"1\"}},\n",
    "    env={\"EXTRA_PIP_PACKAGES\": \"optuna\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ff1fc-fea1-495e-97cb-24e7c3957eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fe9f5-e28d-4c24-9c55-19cbd15038c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118a6b2-771b-4268-9f77-e753062200c2",
   "metadata": {},
   "source": [
    "# Perform hyperparameter optimization with a toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6b3dc-9e09-4fc5-b70d-f9882cf2795c",
   "metadata": {},
   "source": [
    "Now we can run hyperparameter optimization. The workers will run multiple training jobs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f3a02-53f4-47d1-86bf-8a8a729e4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    x = trial.suggest_uniform(\"x\", -10, 10)\n",
    "    return (x - 2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45b6de-d092-457b-b8f3-dd202f04484f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from dask.distributed import wait\n",
    "\n",
    "# Number of hyperparameter combinations to try in parallel\n",
    "n_trials = 100\n",
    "\n",
    "# Optimize in parallel on your Dask cluster\n",
    "backend_storage = optuna.storages.InMemoryStorage()\n",
    "dask_storage = optuna.integration.DaskStorage(storage=backend_storage, client=client)\n",
    "study = optuna.create_study(direction=\"minimize\", storage=dask_storage)\n",
    "\n",
    "futures = []\n",
    "for i in range(0, n_trials, n_workers * 4):\n",
    "    iter_range = (i, min([i + n_workers * 4, n_trials]))\n",
    "    futures.append(\n",
    "        {\n",
    "            \"range\": iter_range,\n",
    "            \"futures\": [\n",
    "                client.submit(study.optimize, objective, n_trials=1, pure=False)\n",
    "                for _ in range(*iter_range)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "for partition in futures:\n",
    "    iter_range = partition[\"range\"]\n",
    "    print(f\"Testing hyperparameter combinations {iter_range[0]}..{iter_range[1]}\")\n",
    "    _ = wait(partition[\"futures\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a138366-4535-4d3f-885e-6066626588aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9437f8-70ef-4649-9068-8f14ef491141",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6bdc73-10fd-4221-abe1-1e04854335e4",
   "metadata": {},
   "source": [
    "# Perform hyperparameter optimization with XGBoost GPU algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ade8a-218f-45ab-bedb-abc4bce5fffc",
   "metadata": {},
   "source": [
    "Now let's try optimizing hyperparameters for an XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5d290-7858-4bd5-8d46-81ce4f1b298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    params = {\n",
    "        \"n_estimators\": 10,\n",
    "        \"verbosity\": 0,\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 100.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 100.0, log=True),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10, step=1),\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        \"min_child_weight\": trial.suggest_float(\n",
    "            \"min_child_weight\", 1e-8, 100, log=True\n",
    "        ),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        # defines how selective algorithm is.\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        \"grow_policy\": \"depthwise\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "    }\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "    fold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    score = cross_val_score(clf, X, y, cv=fold, scoring=\"neg_log_loss\")\n",
    "    return score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9d765-0484-46d7-8728-f1aa25dc5f33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of hyperparameter combinations to try in parallel\n",
    "n_trials = 250\n",
    "\n",
    "# Optimize in parallel on your Dask cluster\n",
    "backend_storage = optuna.storages.InMemoryStorage()\n",
    "dask_storage = optuna.integration.DaskStorage(storage=backend_storage, client=client)\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\", sampler=RandomSampler(seed=0), storage=dask_storage\n",
    ")\n",
    "futures = []\n",
    "for i in range(0, n_trials, n_workers * 4):\n",
    "    iter_range = (i, min([i + n_workers * 4, n_trials]))\n",
    "    futures.append(\n",
    "        {\n",
    "            \"range\": iter_range,\n",
    "            \"futures\": [\n",
    "                client.submit(study.optimize, objective, n_trials=1, pure=False)\n",
    "                for _ in range(*iter_range)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "for partition in futures:\n",
    "    iter_range = partition[\"range\"]\n",
    "    print(f\"Testing hyperparameter combinations {iter_range[0]}..{iter_range[1]}\")\n",
    "    _ = wait(partition[\"futures\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72630515-f7ad-47ce-b57c-b5c6fdd55faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abba0f9-0ca1-41d9-ad8d-eda1a31a26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fdbc4-dd40-49a9-a624-39541475db53",
   "metadata": {},
   "source": [
    "Let's visualize the progress made by hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4951b29-a10d-4a7c-afae-e054a78f89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f37fa0-a8a6-4211-bfc4-2902661c6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a283e5-68c4-41e2-9abe-05af673c3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
