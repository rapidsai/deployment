{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "library/optuna",
     "library/dask",
     "workflow/hpo",
     "library/cuml",
     "library/numpy",
     "dataset/bnp-claims"
    ]
   },
   "source": [
    "# Getting Started with Optuna and RAPIDS for HPO\n",
    "Hyperparameter optimization (HPO) automates the process of picking values for the hyperparameters of a machine learning algorithm to improve model performance. This can help boost the model accuracy, but can be resource-intensive, as it may require training the model for hundreds of hyperparameter combinations. Let's take a look at how we can use Optuna and RAPIDS to make HPO less time-consuming.\n",
    "\n",
    "## RAPIDS\n",
    "The RAPIDS framework provides a suite of libraries to execute end-to-end data science pipelines entirely on GPUs. One of the libraries in this framework is cuML, which implements common machine learning models with a scikit-learn-compatible API and a GPU-accelerated backend. You can learn more about RAPIDS [here](https://rapids.ai/about.html).\n",
    "\n",
    "## Optuna\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/) is a lightweight framework for automatic hyperparameter optimization. It provides a define-by-run API, which makes it easy to adapt to any already existing code that we have and enables high modularity along with the flexibility to construct hyperparameter spaces dynamically. By simply wrapping the objective function with Optuna, we can perform a parallel-distributed HPO search over a search space as we'll see in this notebook.\n",
    "\n",
    "\n",
    "In this notebook, we'll use BNP Paribas Cardif Claims Management dataset from Kaggle to predict if a claim will receive accelerated approval or not. We'll explore how to use Optuna with RAPIDS in combination with Dask to run multi-GPU HPO experiments that can yield results faster than CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to install optuna\n",
    "#!pip install optuna optuna-integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import optuna\n",
    "from cuml import LogisticRegression\n",
    "from cuml.metrics import log_loss\n",
    "from cuml.model_selection import train_test_split\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up CUDA Cluster\n",
    "\n",
    "We start a local cluster and keep it ready for running distributed tasks with dask. The dask scheduler can help leverage multiple nodes available on the cluster.\n",
    "\n",
    "[LocalCUDACluster](https://github.com/rapidsai/dask-cuda) launches one Dask worker for each GPU in the current systems. It's developed as a part of the RAPIDS project. Learn More:\n",
    "- [Setting up Dask](https://docs.dask.org/en/latest/setup.html)\n",
    "- [Dask Client](https://distributed.dask.org/en/latest/client.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-06 09:41:38,254] A new study created in memory with name: dask_optuna_lr_log_loss_tpe\n"
     ]
    }
   ],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1, ip=\"\", dashboard_address=\"8081\")\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "## Data Acquisition\n",
    "Dataset can be acquired from Kaggle: [BNP Paribas Cardif Claims Management](https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/data). To download the dataset:\n",
    " \n",
    "1. Follow the instructions here to: [Set-up the Kaggle API](https://github.com/Kaggle/kaggle-api)\n",
    " \n",
    "2. Run the following to download the data\n",
    "\n",
    "```shell\n",
    "mkdir -p ./data\n",
    "\n",
    "kaggle competitions download \\\n",
    "  -c bnp-paribas-cardif-claims-management \\\n",
    "  --path ./data\n",
    "\n",
    "unzip \\\n",
    "  -d ./data \\\n",
    "  ./data/bnp-paribas-cardif-claims-management.zip\n",
    "```\n",
    " \n",
    "This is an anonymized dataset containing categorical and numerical values for claims received by BNP Paribas Cardif.  The \"target\" column in the train set is the variable to predict. It is equal to 1 for claims suitable for an accelerated approval. The task is to predict whether a claim will be suitable for accelerated approval or not. We'll only use the `train.csv.zip` file as `test.csv.zip` does not have a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_name = \"train.csv.zip\"\n",
    "\n",
    "data_dir = \"data/\"\n",
    "INPUT_FILE = os.path.join(data_dir, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the `N_TRIALS` for the number of runs of HPO trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 150\n",
    "\n",
    "df = cudf.read_csv(INPUT_FILE)\n",
    "\n",
    "# Drop ID column\n",
    "df = df.drop(\"ID\", axis=1)\n",
    "\n",
    "# Drop non-numerical data and fill NaNs before passing to cuML RF\n",
    "CAT_COLS = list(df.select_dtypes(\"object\").columns)\n",
    "df = df.drop(CAT_COLS, axis=1)\n",
    "df = df.fillna(0)\n",
    "\n",
    "df = df.astype(\"float32\")\n",
    "X, y = df.drop([\"target\"], axis=1), df[\"target\"].astype(\"int32\")\n",
    "\n",
    "study_name = \"dask_optuna_lr_log_loss_tpe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "The `train_and_eval` function accepts the different parameters to try out. This function should look very similar to any ML workflow. We'll use this function within the Optuna `objective` function to show how easily we can fit an existing workflow into the Optuna work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(\n",
    "    X_param, y_param, penalty=\"l2\", C=1.0, l1_ratio=None, fit_intercept=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the given data into train and test split to train and evaluate the model\n",
    "    for the params parameters.\n",
    "\n",
    "    Params\n",
    "    ______\n",
    "\n",
    "    X_param:  DataFrame.\n",
    "              The data to use for training and testing.\n",
    "    y_param:  Series.\n",
    "              The label for training\n",
    "    penalty, C, l1_ratio, fit_intercept: The parameter values for Logistic Regression.\n",
    "\n",
    "    Returns\n",
    "    score: log loss of the fitted model\n",
    "    \"\"\"\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_param, y_param, random_state=42\n",
    "    )\n",
    "    classifier = LogisticRegression(\n",
    "        penalty=penalty,\n",
    "        C=C,\n",
    "        l1_ratio=l1_ratio,\n",
    "        fit_intercept=fit_intercept,\n",
    "        max_iter=10000,\n",
    "    )\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    score = log_loss(y_valid, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline number, let's see what the default performance of the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [09:34:11.132560] L-BFGS line search failed (code 3); stopping at the last valid step\n",
      "Score with default parameters :  8.24908383066997\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with default parameters : \", train_and_eval(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    " \n",
    "We will optimize the objective function using [Optuna Study](https://optuna.readthedocs.io/en/stable/reference/study.html). The objective function tries out specified values for the parameters that we are tuning and returns the score obtained with those parameters. These results will be aggregated in `study.trials_dataframes()`. \n",
    " \n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    " \n",
    "The objective function does not need to be changed when switching to different [samplers](https://optuna.readthedocs.io/en/stable/reference/samplers.html), which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll use TPESampler for this demo, but feel free to try different samplers to notice the changes in performance. \n",
    " \n",
    "\n",
    "[Tree-Structured Parzen Estimators](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html#optuna.samplers.TPESampler) or TPE works by fitting two  Gaussian Mixture Model during each trial - one to the set of parameter values associated with the best objective values,\n",
    "and another to the remaining parameter values. It chooses the parameter value that maximizes the ratio between the two GMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_param, y_param):\n",
    "    C = trial.suggest_float(\"C\", 0.01, 100.0, log=True)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"none\", \"l1\", \"l2\"])\n",
    "    fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "\n",
    "    score = train_and_eval(\n",
    "        X_param, y_param, penalty=penalty, C=C, fit_intercept=fit_intercept\n",
    "    )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Trials and Study\n",
    " \n",
    "Optuna uses [studies](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. Put simply, a trial is a single call of the objective function while a set of trials make up a study. We will pick the best observed trial from a study to get the best parameters that were used in that run.\n",
    "\n",
    "Here, `DaskStorage` class is used to set up a storage shared by all workers in the cluster. Learn more about what storages can be used [here](https://optuna.readthedocs.io/en/stable/tutorial/distributed.html)\n",
    "\n",
    "`optuna.create_study` is used to set up the study. As you can see, it specifies the study name, sampler to be used, the direction of the study, and the storage.\n",
    "With just a few lines of code, we have set up a distributed HPO experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage = optuna.integration.DaskStorage()\n",
    "study = optuna.create_study(\n",
    "    sampler=optuna.samplers.TPESampler(seed=142),\n",
    "    study_name=study_name,\n",
    "    direction=\"minimize\",\n",
    "    storage=storage,\n",
    ")\n",
    "\n",
    "# Optimize in parallel on your Dask cluster\n",
    "#\n",
    "# Submit `n_workers` optimization tasks, where each task runs about 40 optimization trials\n",
    "# for a total of about N_TRIALS trials in all\n",
    "futures = [\n",
    "    c.submit(\n",
    "        study.optimize,\n",
    "        lambda trial: objective(trial, X, y),\n",
    "        n_trials=N_TRIALS // n_workers,\n",
    "        pure=False,\n",
    "    )\n",
    "    for _ in range(n_workers)\n",
    "]\n",
    "wait(futures)\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see logs like the following.\n",
    "\n",
    "```text\n",
    "[I 2024-08-06 09:41:40,161] Trial 1 finished with value: 8.238207899472073 and parameters: {'C': 40.573838784392514, 'penalty': 'l2', 'fit_intercept': True}. Best is trial 1 with value: 8.238207899472073.\n",
    "... \n",
    "[I 2024-08-06 09:41:58,423] Trial 143 finished with value: 8.210414278942531 and parameters: {'C': 0.3152731188939818, 'penalty': 'l1', 'fit_intercept': True}. Best is trial 52 with value: 8.205579602300705.\n",
    "\n",
    "Best params: {'C': 1.486491072441749, 'penalty': 'l2', 'fit_intercept': True}\n",
    "Number of finished trials:  144\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Optuna provides an easy way to visualize the trials via builtin graphs. Read more about visualizations [here](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conluding Remarks\n",
    " \n",
    "This notebook shows how RAPIDS and Optuna can be used along with dask to run multi-GPU HPO jobs, and can be used as a starting point for anyone wanting to get started with the framework. We have seen how by just adding a few lines of code we were able to integrate the libraries for a muli-GPU HPO runs. This can also be scaled to multiple nodes.\n",
    " \n",
    "## Next Steps\n",
    " \n",
    "This is done on a small dataset, you are encouraged to test out on larger data with more range for the parameters too. These experiments can yield performance improvements. Refer to other examples in the [rapidsai/cloud-ml-examples](https://github.com/rapidsai/cloud-ml-examples) repository.\n",
    " \n",
    "## Resources\n",
    "[Hyperparameter Tuning in Python](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624)\n",
    "\n",
    "[Overview of Hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview)\n",
    "\n",
    "[How to make your model awesome with Optuna](https://towardsdatascience.com/how-to-make-your-model-awesome-with-optuna-b56d490368af)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
